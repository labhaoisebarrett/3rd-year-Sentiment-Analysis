{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMBD dataset holds 50,000 movie reviews. They are highly polarised and categorised into positive and negative reviews (25000 each). The task is binary sentiment classification. Given a review can a model predict whether it is being positive or negative about the movie?\n",
    "\n",
    "Most of the skills used here come from these DataCamp courses: \n",
    "- Recurrent Neural Networks for Language Modeling in Python\n",
    "- Introduction to Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "#df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print (len(df[df['sentiment']=='negative']))\n",
    "print (len(df[df['sentiment']=='positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is balanced between negative and positive reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Keras and NLTK can be used to get the text data ready to be fed into a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has the `text_to_word_sequence` function that converts a text block into a list of words. It also filters out punctuation and converts text to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [text_to_word_sequence(phrase) for phrase in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to make sure that only words are kept in the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetic_reviews = list()\n",
    "for review in reviews: \n",
    "    alphabetic_review = [word for word in review if word.isalpha() and word != 'br']\n",
    "    alphabetic_reviews.append(alphabetic_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are commonly used words that don't add to the meaning of the phrase - so it won't help our model predict sentiment. NLTK has a list of stopwords which can be used to filter the reviews of this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_reviews = list()\n",
    "for review in alphabetic_reviews: \n",
    "    filtered_review = [word for word in review if not word in stop_words]\n",
    "    filtered_reviews.append(filtered_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK `WordNetLemmatizer` reduces inflected words to its root word. Examples of this include:\n",
    "- 'Cats' becoming 'cat'\n",
    "- Corpora' becoming 'corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lemmatizer to work, it needs to know what type of word it's processing. NLTK's `pos_tag` returns the POS (Part Of Speech) that word falls under, e.g. verb. This helps the lemmatizer know how to lemmatize the word. But the lemmatizer takes a different kind of POS tag so it has to be converted from an NLTK tag to a WordNet tag. This is done using a function (derived from https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258) :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then another function can be created to lemmatize each review. First the words of each review are tagged as an NLTK Part Of Speech. Then these tags are converted to WordNet tags. Once a dictionary of words and tags are created, the lemmatizer can run on each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(review):\n",
    "    #get nltk tag\n",
    "    nltk_tag = nltk.pos_tag(review)  \n",
    "    #create dict of words and associated wordnet tag \n",
    "    wordnet_tag = dict()\n",
    "    for word,tag in nltk_tag: \n",
    "        wordnet_tag[word] = nltk_tag_to_wordnet_tag[word[1]]\n",
    "        \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tag:\n",
    "        if tag is None:\n",
    "            #no tag means can't lemmatize\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function on reviews\n",
    "lemmatized_reviews = list()\n",
    "for review in filtered_reviews: \n",
    "    lemmatized_reviews.append(lemmatize_sentence(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lemmatized_reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each review has been cleaned up. But it's still not ready to be used in a ML model. The next step is to represent the words as a collection of vectors. A Keras Tokenizer can be used to do this. First the reviews are converted to sequences. Then the vectors need to all be same length to be used in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lemmatized_reviews)\n",
    "#convert words to numbers\n",
    "features = tokenizer.texts_to_sequences(lemmatized_reviews)\n",
    "#add zeros to arrays until they are all the same length \n",
    "features = sequence.pad_sequences(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are now ready to be used in a model. Next we need a target. Luckily this is a much simpler task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.get_dummies(df['sentiment'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning, as the name suggests, involves transferring information from an already developed model to a new one. It means rather than the model starting with randomly chosen weights, it is initialised with better weights. \n",
    "\n",
    "GloVE (Global Vectors for Word Representation) is an example of this for language models. It was developed at Stanford. \n",
    "\n",
    "Using functions from DataCamp (Recurrent Neural Networks for Language Modeling in Python) the GloVE pretrained vectors can be extracted and it can be set up to be used with this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary dict holds the list of words collected by the tokenizer and the number it assigned to it \n",
    "vocabulary_dict = tokenizer.word_index\n",
    "vocabulary_size = len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove file \n",
    "file = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get all the words and associated vectors from the glove file \n",
    "glove_vector_dict = dict()\n",
    "with open(file) as f: \n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = values[1:]\n",
    "        glove_vector_dict[word] = np.asarray(coefs,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then create a matrix specific to this dataset by looking up words in the glove dictionary\n",
    "#and adding their already established vector to an array \n",
    "embedding_matrix = np.zeros((vocabulary_size+1, 300))\n",
    "for word, i in vocabulary_dict.items():\n",
    "    embedding_vector = glove_vector_dict.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data has been cleaned and translated into numbers and initial weights have been established. Now it's time to actually create a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "An embedding layer creates vectors in such a way that:\n",
    "- It uses less dimensions than one hot encoding (one hot encoding would create very sparse vectors when dealing with the many unique words of this task).\n",
    "- It puts similar features closer together because they attempt to learn something about the data when they work. An embedding will, for instance, put positive words closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = features.shape[1]\n",
    "output_dim = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add embedding layer using the embedding matrix from glove \n",
    "model.add(Embedding(input_dim=vocabulary_size + 1,\n",
    "                     output_dim=output_dim, weights = [embedding_matrix], input_length=input_length)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation \n",
    "Normalising the neural network will help its performance. It will help the network train more quickly because it should converge more quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs \n",
    "RNNs use the information that simple neural networks do but they also learn the context of their predictions. This is useful in sentiment analysis - the words that preceed or follow a given word can change its sentiment. \n",
    "\n",
    "In the case of sentiment analysis a 'many to one' RNN would be used. This takes a sequence of inputs and generates a single output. However, simple RNNs aren't used very often in practice. The main reason for this is the vanishing/exploding gradient problems. \n",
    "\n",
    "The vanishing gradient problem : simple RNNs don't perform well on long series of data. In this case the gradient will become vanishingly small and so the model won't be able to use it to improve its error score. \n",
    "\n",
    "Exploding gradient problem: over time large error gradients can be accumulated by an RNN. This results in very large updates to the weights and this makes the model perform poorly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs \n",
    "Long Short Term Memory networks (LSTMs) are a type of RNN. They function better than a simple RNN for most sequence problems. They are more selective in what they remember in order to predict the future. \n",
    "\n",
    "They solve the vanishing and exploding gradient problems because they only remember important information. \n",
    "\n",
    "The difference between an these networks and a simple RNN are the operations in the LSTM cells. They contain mechanisms called gates that control the flow of information. These changes allow the network to decide whether to keep or forget information based on importance. The gates contain sigmoid activations. This keeps values between 0 and 1. Here 0 means values that should be forgotten and 1 means values that should be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(6, return_sequences = False))                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer\n",
    "Dropout involves randomly choosing some neurons to ignore during training. This helps the model be less sensitive to the specific weights of neurons and should help the model not  overfit on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_108 (Embedding)    (None, 1401, 300)         26229300  \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1401, 300)         1200      \n",
      "_________________________________________________________________\n",
      "lstm_106 (LSTM)              (None, 6)                 7368      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 26,237,875\n",
      "Trainable params: 26,237,275\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping helps prevent overfitting \n",
    "early_stopping_monitor = EarlyStopping(patience=2,monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/3\n",
      "35000/35000 [==============================] - 1258s 36ms/step - loss: 0.5394 - accuracy: 0.7153 - val_loss: 0.3569 - val_accuracy: 0.8506\n",
      "Epoch 2/3\n",
      "35000/35000 [==============================] - 1262s 36ms/step - loss: 0.3261 - accuracy: 0.8659 - val_loss: 0.3293 - val_accuracy: 0.8619\n",
      "Epoch 3/3\n",
      "35000/35000 [==============================] - 1328s 38ms/step - loss: 0.2529 - accuracy: 0.9019 - val_loss: 0.3299 - val_accuracy: 0.8633\n"
     ]
    }
   ],
   "source": [
    "model.fit(features, target, validation_split=0.3, epochs=3,\n",
    "          verbose=True,callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions determine the output of a neural network through maths equations. An activation function is applied to the output of a neuron. If a model doesn't have an activation function it is essentially just a linear regression model. This prevents the model from learning complex patterns in the data. \n",
    "\n",
    "Three activation functions to consider: \n",
    "- Sigmoid: varies between 0 and 1 for all possible X input values \n",
    "- Tanh: varies between -1 and 1\n",
    "- ReLU: varies between 0 and infinity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch is a sample of data. Batch size is a hyperparameter that controls how many samples a model will work through in each epoch before the internal parameters are updated. As a rule of thumb the bigger a dataset, the bigger the batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is the recommended optimizer for most deep learning problems. Its learning rate is a very important hyperparameter. It controls how much the model changes in response to estimated error each time the model weights update. If the learning rate is too small the training process may take too long whereas a too large value could lead to the model choosing poor weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "I ran cross-validation on a smaller subset of the dataset and the best parameters that were found were: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'learning_rate': 0.01, 'batch_size': 256, 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed for reproducibility \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "    opt = keras.optimizers.Adam(lr = learning_rate)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=output_dim, weights = [embedding_matrix], input_length=input_length)) \n",
    "    model.add(Dense(6, activation = activation))\n",
    "    model.add(LSTM(6, return_sequences = False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation='sigmoid', name = 'output'))\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_model = KerasClassifier(build_fn = create_model,verbose=0)\n",
    "\n",
    "params = {'activation': ['sigmoid','relu', 'tanh'], 'batch_size': [32, 128, 256],\n",
    "          'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "#search for parameters that give lowest validation loss\n",
    "random_search = RandomizedSearchCV(tuning_model, param_distributions = params, cv = KFold(3),estimator='val_loss')\n",
    "\n",
    "random_search.fit(features,target)\n",
    "print (random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using deep learning for sentiment classification is known to be effective. Here our model has learnt about movie reviews using these steps:\n",
    "1. Translating the text of the reviews into numbers \n",
    "2. Using GLoVe embeddings to give the model a better than random starting point \n",
    "3. Building a deep learning model using LSTM, Dropout, BatchNormalization and Dense layers.\n",
    "4. Evaluating this model using cross validation to find the best hyperparameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
